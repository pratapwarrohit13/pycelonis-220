"""Module to interact with data pool tables.

This module contains class to interact with data pool tables in EMS data integration.

Typical usage example:

    ```python
    tables = data_pool.get_tables()
    data_pool_table = data_pool.create_table(df, "TEST_TABLE")
    data_pool_table.append(df)
    data_pool_table.upsert(df, keys=["PRIMARY_KEY_COLUMN"])
    ```
"""
import logging
import typing

import pandas as pd
from pycelonis.service.integration.service import IntegrationService, JobType, PoolColumn, PoolTable
from pycelonis_core.base.collection import CelonisCollection
from pycelonis_core.client.client import Client
from pycelonis_core.utils.errors import PyCelonisNotFoundError
from pydantic.v1 import Field

logger = logging.getLogger(__name__)


class DataPoolTable(PoolTable):
    """Data model table object to interact with data model table specific data integration endpoints."""

    client: Client = Field(..., exclude=True)
    name: str
    """Name of data pool table."""
    data_pool_id: str
    """Id of data pool where table is located."""
    data_source_id: typing.Optional[str]
    """Id of data connection where table is located."""
    columns: typing.Optional[typing.List[typing.Optional[PoolColumn]]]
    """Columns of data pool table."""

    @classmethod
    def from_transport(cls, client: Client, data_pool_id: str, pool_table_transport: PoolTable) -> "DataPoolTable":
        """Creates high-level data pool table object from given PoolTable.

        Args:
            client: Client to use to make API calls for given data pool table.
            data_pool_id: Id of data pool where table is located
            pool_table_transport: PoolTable object containing properties of data pool table.

        Returns:
            A DataPoolTable object with properties from transport and given client.
        """
        return cls(client=client, data_pool_id=data_pool_id, **pool_table_transport.dict())

    def sync(self) -> None:
        """Syncs data pool table properties with EMS."""
        for table in IntegrationService.get_api_pools_id_tables(self.client, self.data_pool_id):
            if table is not None and table.name == self.name and table.data_source_id == self.data_source_id:
                self._update(table)
                return

        raise PyCelonisNotFoundError(f"Data pool table with name '{self.name}' no longer exists.")

    def __getter_attributes__(self) -> typing.List[str]:
        return ["columns"]

    def __main_attributes__(self) -> typing.Optional[typing.List[str]]:
        return ["name", "data_pool_id", "data_source_id", "schema_name", "columns"]

    ############################################################
    # Column
    ############################################################
    def get_columns(self) -> CelonisCollection[typing.Optional["PoolColumn"]]:
        """Gets all table columns of given table.

        Returns:
            A list containing all columns of table.
        """
        logger.warning(
            "For string columns, the field length is automatically multiplied by 4. For creating new tables based on "
            "the given columns, make sure to divide the field length for each column by 4 again in the column config "
            "before pushing."
        )
        return CelonisCollection(
            IntegrationService.get_api_pools_id_columns(
                self.client, self.data_pool_id, table_name=self.name, schema_name=self.data_source_id
            )
        )

    ############################################################
    # Push Table
    ############################################################
    def upsert(
        self,
        df: pd.DataFrame,
        keys: typing.List[str],
        chunk_size: int = 100_000,
        index: typing.Optional[bool] = False,
        **kwargs: typing.Any,
    ) -> None:
        """Upserts data frame to existing table in data pool.

        Args:
            df: DataFrame to push to existing table.
            keys: Primary keys of table.
            chunk_size: Number of rows to push in one chunk.
            index: Whether index is included in parquet file that is pushed. Default False. See
                [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html).
            **kwargs: Additional parameters set for
                [DataPushJob][pycelonis.service.integration.service.DataPushJob] object.

        Returns:
            The updated table object.

        Raises:
            PyCelonisTableDoesNotExistError: Raised if table does not exist in data pool.
            PyCelonisDataPushExecutionFailedError: Raised when table creation fails.

        Examples:
            Upsert new data to table:
            ```python
            df = pd.DataFrame({"ID": ["aa", "bb", "cc"], "TEST_COLUMN": [1,2, 3]})

            pool_table = data_pool.get_table("TEST_TABLE")
            data_pool_table.upsert(df, keys=["ID"])
            ```
        """
        self._push_data_frame(df, keys=keys, chunk_size=chunk_size, index=index, **kwargs)
        logger.info("Successfully upserted rows to table '%s' in data pool", self.name)

    def append(
        self,
        df: pd.DataFrame,
        chunk_size: int = 100_000,
        index: typing.Optional[bool] = False,
        **kwargs: typing.Any,
    ) -> None:
        """Appends data frame to existing table in data pool.

        Args:
            df: DataFrame to push to existing table.
            chunk_size: Number of rows to push in one chunk.
            index: Whether index is included in parquet file that is pushed. Default False. See
                [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html).
            **kwargs: Additional parameters set for
                [NewTaskInstanceTransport][pycelonis.service.integration.service.NewTaskInstanceTransport] object.

        Returns:
            The updated table object.

        Raises:
            PyCelonisTableDoesNotExistError: Raised if table does not exist in data pool
            PyCelonisDataPushExecutionFailedError: Raised when table creation fails

        Examples:
            Append new data to table:
            ```python
            df = pd.DataFrame({"ID": ["aa", "bb", "cc"], "TEST_COLUMN": [1,2, 3]})

            pool_table = data_pool.get_table("TEST_TABLE")
            data_pool_table.append(df)
            ```
        """
        self._push_data_frame(df, keys=None, chunk_size=chunk_size, index=index, **kwargs)
        logger.info("Successfully appended rows to table '%s' in data pool", self.name)

    def _push_data_frame(
        self,
        df: pd.DataFrame,
        keys: typing.Optional[typing.List[str]] = None,
        chunk_size: int = 100_000,
        index: typing.Optional[bool] = False,
        **kwargs: typing.Any,
    ) -> None:
        from pycelonis.ems.data_integration.data_pool import DataPool

        data_push_job = DataPool.create_data_push_job_from(
            client=self.client,
            data_pool_id=self.data_pool_id,
            target_name=self.name,
            type_=JobType.DELTA,
            keys=keys,
            connection_id=self.data_source_id,
            **kwargs,
        )
        try:
            data_push_job.add_data_frame(df, chunk_size=chunk_size, index=index)
            data_push_job.execute()
        finally:
            data_push_job.delete()

