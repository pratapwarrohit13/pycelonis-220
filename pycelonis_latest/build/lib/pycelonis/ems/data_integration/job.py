"""Module to interact with jobs.

This module contains class to interact with a job in EMS data integration.

Typical usage example:

```python
job = data_pool.get_job(
    job_id
)
job.name = (
    "NEW_NAME"
)
job.update()

job.execute()

data_model_execution = job.get_data_model_execution(
    "<EXECUTION_ID_HERE>"
)

job.execute(
    transformation_ids=[
        transformation.id
    ],
    data_model_execution_configurations=[
        DataModelExecutionConfiguration(
            data_model_execution_id=data_model_execution.id,
            tables=[
                t.id
                for t in data_model_execution.tables
            ],
        )
    ],
)

job.delete()
```
"""

import logging
import typing

from pycelonis.config import Config
from pycelonis.ems.data_integration.data_model_execution import DataModelExecution
from pycelonis.ems.data_integration.task import Task
from pycelonis.errors import PyCelonisExecutionFailedError, PyCelonisExecutionInProgressError
from pycelonis.service.integration.service import (
    DataModelExecutionConfiguration,
    DataModelExecutionTableItem,
    DataModelExecutionTransport,
    EntityStatus,
    ExecutionItemTransport,
    ExecutionStatus,
    ExecutionType,
    ExtractionConfiguration,
    ExtractionMode,
    IntegrationService,
    JobCopyRequestTransport,
    JobExecutionConfiguration,
    JobTransport,
    NewTaskInstanceTransport,
    TaskType,
)
from pycelonis.utils.polling import poll
from pycelonis_core.base.collection import CelonisCollection
from pycelonis_core.client.client import Client
from pycelonis_core.utils.errors import PyCelonisNotFoundError

try:
    from pydantic.v1 import Field  # type: ignore
except ImportError:
    from pydantic import Field  # type: ignore

logger = logging.getLogger(__name__)


class Job(JobTransport):
    """Data job object to interact with data job specific data integration endpoints."""

    client: Client = Field(..., exclude=True)
    id: str
    """Id of job."""
    data_pool_id: str
    """Id of data pool where job is located."""
    data_source_id: typing.Optional[str]
    """Id of data connection where job is located."""
    name: typing.Optional[str]
    """Name of job."""

    @classmethod
    def from_transport(cls, client: Client, job_transport: JobTransport) -> "Job":
        """Creates high-level job object from given JobTransport.

        Args:
            client: Client to use to make API calls for given job.
            job_transport: JobTransport object containing properties of job.

        Returns:
            A Job object with properties from transport and given client.
        """
        return cls(client=client, **job_transport.dict())

    def update(self) -> None:
        """Pushes local changes of job to EMS and updates properties with response from EMS."""
        updated_job = IntegrationService.put_api_pools_pool_id_jobs_id(self.client, self.data_pool_id, self.id, self)
        logger.info("Successfully updated job with id '%s'", self.id)
        self._update(updated_job)

    def sync(self) -> None:
        """Syncs job properties with EMS."""
        synced_job = IntegrationService.get_api_pools_pool_id_jobs_id(self.client, self.data_pool_id, self.id)
        self._update(synced_job)

    def delete(self) -> None:
        """Deletes job."""
        IntegrationService.delete_api_pools_pool_id_jobs_job_id(self.client, self.data_pool_id, self.id)
        logger.info("Successfully deleted job with id '%s'", self.id)

    def copy_to(
        self,
        destination_team_domain: str,
        destination_data_pool_id: str,
        destination_data_source_id: typing.Optional[str] = None,
        **kwargs: typing.Any,
    ) -> "JobTransport":
        """Copies data job to the specified domain in the same realm.

        Args:
            destination_team_domain: The <team-name> of the destination
                team url, https://<team-name>.<realm>.celonis.cloud/
            destination_data_pool_id: The id of the destination data pool.
            destination_data_source_id: (Optional) The id of the destination data connection.
                By default, the global data source is used.
            **kwargs: Additional parameters set for
                [JobCopyRequestTransport] [pycelonis.service.integration.service.JobCopyRequestTransport]

        Returns:
            A read-only job transport object of the copied asset.

        Examples:
            ```python
            job_copy = job.copy_to(
                "celonis-team-domain",
                "zd6a18r1-171e-4f94-bdf1-c58b61251d37",
                "bd2e1854-851e-4a96-s0f5-d08z68423e48",
            )
            ```
        """
        job_transport = IntegrationService.post_api_pools_pool_id_jobs_job_id_copy(
            self.client,
            self.data_pool_id,
            self.id,
            JobCopyRequestTransport(
                destination_team_domain=destination_team_domain,
                destination_pool_id=destination_data_pool_id,
                destination_data_source_id=destination_data_source_id,
                **kwargs,
            ),
        )
        logger.info("Successfully copied to job with id '%s'", job_transport.id)
        return job_transport

    def __main_attributes__(self) -> typing.Optional[typing.List[str]]:
        return ["id", "name", "data_pool_id"]

    ############################################################
    # Task
    ############################################################
    def create_task(
        self,
        name: str,
        task_type: TaskType,
        description: typing.Optional[str] = None,
        **kwargs: typing.Any,
    ) -> "Task":
        """Creates new task with name in given data job.

        Args:
            name: Name of new task.
            task_type: Type of new task.
            description: Description of new task.
            **kwargs: Additional parameters set for
                [NewTaskInstanceTransport][pycelonis.service.integration.service.NewTaskInstanceTransport] object.

        Returns:
            A Task object for newly created task.
        """
        task_transport = IntegrationService.post_api_pools_pool_id_jobs_job_id_tasks(
            self.client,
            self.data_pool_id,
            self.id,
            NewTaskInstanceTransport(
                name=name,
                job_id=self.id,
                task_type=task_type,
                description=description,
                **kwargs,
            ),
        )
        logger.info(
            "Successfully created task of type '%s' with id '%s'",
            task_type,
            task_transport.id,
        )
        return Task.from_transport(self.client, task_transport)

    def create_transformation(
        self, name: str, description: typing.Optional[str] = None, **kwargs: typing.Any
    ) -> "Task":
        r"""Creates new transformation task with name in given data job.

        Args:
            name: Name of new transformation task.
            description: Description of new transformation task
            **kwargs: Additional parameters set for
                [NewTaskInstanceTransport][pycelonis.service.integration.service.NewTaskInstanceTransport] object.

        Returns:
            A Task object for newly created transformation task.

        Examples:
            Create data job with transformation statement and execute it:
            ```python
            data_job = data_pool.create_job("PyCelonis Tutorial Job")

            task = data_job.create_transformation(
                name="PyCelonis Tutorial Task",
                description="This is an example task"
            )

            task.update_statement(\"\"\"
                DROP TABLE IF EXISTS ACTIVITIES;
                CREATE TABLE ACTIVITIES (
                    _CASE_KEY VARCHAR(100),
                    ACTIVITY_EN VARCHAR(300)
                );
            \"\"\")

            data_job.execute()
            ```
        """
        return self.create_task(name, TaskType.TRANSFORMATION, description=description, **kwargs)  # type: ignore

    def create_extraction(self, name: str, **kwargs: typing.Any) -> "Task":
        """Creates new extraction task with name in given data job.

        Args:
            name: Name of new extraction task.
            **kwargs: Additional parameters set for
                [NewTaskInstanceTransport][pycelonis.service.integration.service.NewTaskInstanceTransport] object.

        Returns:
            A Task object for newly created extraction task.
        """
        return self.create_task(name, TaskType.EXTRACTION, **kwargs)  # type: ignore

    def get_task(self, id_: str) -> "Task":
        """Gets task with given id.

        Args:
            id_: Id of task.

        Returns:
            A Task object for task with given id.
        """
        for task in self.get_tasks():
            if task.id == id_:
                return Task.from_transport(self.client, task)

        raise PyCelonisNotFoundError(f"No task with id '{id_}' found in data model.")

    def get_tasks(self) -> "CelonisCollection[Task]":
        """Gets all tasks of given data job.

        Returns:
            A list containing all tasks.
        """
        task_transports = IntegrationService.get_api_pools_pool_id_jobs_job_id_tasks(
            self.client, self.data_pool_id, self.id
        )
        return CelonisCollection(
            Task.from_transport(self.client, task_transport)
            for task_transport in task_transports
            if task_transport is not None
        )

    def get_transformations(self) -> "CelonisCollection[Task]":
        """Gets all transformations of given data job.

        Returns:
            A list containing all transformations.
        """
        return self.get_tasks().find_all(TaskType.TRANSFORMATION, "task_type")

    def get_extractions(self) -> "CelonisCollection[Task]":
        """Gets all extractions of given data job.

        Returns:
            A list containing all extractions.
        """
        return self.get_tasks().find_all(TaskType.EXTRACTION, "task_type")

    ############################################################
    # Job Execution
    ############################################################
    def execute(
        self,
        transformation_ids: typing.Optional[typing.List[str]] = None,
        extraction_configurations: typing.Optional[typing.List[ExtractionConfiguration]] = None,
        data_model_execution_configurations: typing.Optional[typing.List[DataModelExecutionConfiguration]] = None,
        mode: ExtractionMode = ExtractionMode.DELTA,  # type: ignore
        wait: bool = True,
        **kwargs: typing.Any,
    ) -> None:
        r"""Executes job with given transformations, extractions, and data model executions.

        Args:
            transformation_ids: Ids of transformations to use. Default is None which executes all transformations.
            extraction_configurations: Extraction configurations to define which extractions to execute. Default is None
                which executes all extractions.
            data_model_execution_configurations: Data model execution configurations to define which data model reloads
                to execute. Default is None which executes all data model reloads. If configuration is given it needs
                to contain all table ids to reload.
            mode: Extraction mode. Default is DELTA.
            wait: If true, function only returns once data job has been executed and raises error if it fails. It does
                only wait until the job has finished and not until all data models have been reloaded.
                If false, function returns after triggering execution and does not raise errors in case it failed.
            **kwargs: Additional parameters set for
                [JobExecutionConfiguration][pycelonis.service.integration.service.JobExecutionConfiguration] object.

        Examples:
            Create data job with transformation statement and execute it:
            ```python
            data_job = data_pool.create_job("PyCelonis Tutorial Job")

            task = data_job.create_transformation(
                name="PyCelonis Tutorial Task",
                description="This is an example task"
            )

            task.update_statement(\"\"\"
                DROP TABLE IF EXISTS ACTIVITIES;
                CREATE TABLE ACTIVITIES (
                    _CASE_KEY VARCHAR(100),
                    ACTIVITY_EN VARCHAR(300)
                );
            \"\"\")

            data_job.execute()
            ```
        """
        self._verify_execution_possible()

        self._trigger_execution(
            transformation_ids=transformation_ids,
            extraction_configurations=extraction_configurations,
            data_model_execution_configurations=data_model_execution_configurations,
            mode=mode,
            **kwargs,
        )

        if wait:
            self._wait_for_execution()
            self._verify_execution_successful()

    def cancel_execution(self) -> None:
        """Cancels the execution of the job."""
        IntegrationService.post_api_pools_pool_id_jobs_job_id_cancel(self.client, self.data_pool_id, self.id)
        logger.info("Successfully cancelled execution for job with id '%s'", self.id)

    def get_current_execution_status(self) -> EntityStatus:
        """Gets the current execution status of the job."""
        for execution_status in IntegrationService.get_api_pools_pool_id_logs_status(self.client, self.data_pool_id):
            if execution_status is not None and execution_status.id == self.id:
                return execution_status

        raise PyCelonisNotFoundError(f"No execution status logs found for job with id {self.id}")

    def _trigger_execution(
        self,
        transformation_ids: typing.Optional[typing.List[str]],
        extraction_configurations: typing.Optional[typing.List[ExtractionConfiguration]],
        data_model_execution_configurations: typing.Optional[typing.List[DataModelExecutionConfiguration]],
        mode: ExtractionMode,
        **kwargs: typing.Any,
    ) -> None:
        execute_only_subset_of_transformations = transformation_ids is not None
        transformation_ids = transformation_ids or []

        execute_only_subset_of_extractions = extraction_configurations is not None
        extraction_configurations = extraction_configurations or []

        load_only_subset_of_data_models = data_model_execution_configurations is not None
        data_model_execution_configurations = data_model_execution_configurations or []

        job_execution_configuration = JobExecutionConfiguration(
            pool_id=self.data_pool_id,
            job_id=self.id,
            mode=mode,
            execute_only_subset_of_transformations=execute_only_subset_of_transformations,
            transformations=transformation_ids,
            execute_only_subset_of_extractions=execute_only_subset_of_extractions,
            extractions=extraction_configurations,
            load_only_subset_of_data_models=load_only_subset_of_data_models,
            data_models=data_model_execution_configurations,
            **kwargs,
        )

        IntegrationService.post_api_pools_pool_id_jobs_job_id_execute(
            self.client, self.data_pool_id, self.id, job_execution_configuration
        )
        logger.info("Successfully started execution for job with id '%s'", self.id)

    def _verify_execution_possible(self) -> None:
        if self._is_execution_in_progress(self.get_current_execution_status()):
            raise PyCelonisExecutionInProgressError

    def _wait_for_execution(self) -> None:
        def is_execution_done(current_execution_status: EntityStatus) -> bool:
            return not self._is_execution_in_progress(current_execution_status)

        def format_current_execution_status(
            current_execution_status: EntityStatus,
        ) -> str:
            formatted_execution_status = "Status:"
            if current_execution_status.status:
                formatted_execution_status += f" {current_execution_status.status}"
            return formatted_execution_status

        logger.info("Wait for execution of job with id '%s'", self.id)
        poll(
            target=self.get_current_execution_status,
            wait_for=is_execution_done,
            message=format_current_execution_status,
            sleep=Config.POLLING_WAIT_SECONDS,
        )

    def _verify_execution_successful(self) -> None:
        current_execution_status = self.get_current_execution_status()
        if current_execution_status.status != ExecutionStatus.SUCCESS:
            detailed_error_log = self._get_execution_detailed_error_log()
            str_detailed_error_log = "\n".join(detailed_error_log)
            if len(str_detailed_error_log) == 0:
                error_message = str(current_execution_status.status) if current_execution_status.status else ""
            else:
                error_message = str_detailed_error_log
            raise PyCelonisExecutionFailedError(error_message)

    def _is_execution_in_progress(self, current_execution_status: EntityStatus) -> bool:
        return current_execution_status.status in [
            ExecutionStatus.RUNNING,
            ExecutionStatus.QUEUED,
        ]

    ############################################################
    # Data Model Execution
    ############################################################
    def create_data_model_execution(
        self,
        data_model_id: str,
        table_ids: typing.Optional[typing.List[str]] = None,
        **kwargs: typing.Any,
    ) -> "DataModelExecution":
        """Creates data model execution in given data job.

        Args:
            data_model_id: Id of data model to reload.
            table_ids: Ids of tables to reload. Defaults to full load.
            **kwargs: Additional parameters set for
                [DataModelExecutionTransport][pycelonis.service.integration.service.DataModelExecutionTransport] object.

        Returns:
            A DataModelExecution object for newly created data model execution.
        """
        tables = [DataModelExecutionTableItem(id=table_id) for table_id in table_ids] if table_ids else None
        partial_load = table_ids is not None

        data_model_execution_transport = IntegrationService.post_api_pools_pool_id_jobs_job_id_loads(
            self.client,
            self.data_pool_id,
            self.id,
            DataModelExecutionTransport(
                data_model_id=data_model_id,
                job_id=self.id,
                tables=tables,
                partial_load=partial_load,
                **kwargs,
            ),
        )
        logger.info(
            "Successfully created data model execution with id '%s'",
            data_model_execution_transport.id,
        )

        # Additional getter call necessary as POST does not return all properties
        return self.get_data_model_execution(data_model_execution_transport.id)

    def get_data_model_execution(self, id_: typing.Optional[str]) -> "DataModelExecution":
        """Gets data model execution with given id.

        Args:
            id_: Id of data model execution.

        Returns:
            A DataModelExecution object for data model execution with given id.
        """
        for data_model_execution in self.get_data_model_executions():
            if data_model_execution.id == id_:
                return data_model_execution

        raise PyCelonisNotFoundError(f"No data model execution with id '{id_}' found in data job.")

    def get_data_model_executions(self) -> "CelonisCollection[DataModelExecution]":
        """Gets all data model executions of given data job.

        Returns:
            A list containing all data model executions.
        """
        data_model_execution_transports = IntegrationService.get_api_pools_pool_id_jobs_job_id_loads(
            self.client, self.data_pool_id, self.id
        )
        return CelonisCollection(
            DataModelExecution.from_transport(self.client, self.data_pool_id, data_model_execution_transport)
            for data_model_execution_transport in data_model_execution_transports
            if data_model_execution_transport is not None
        )

    ############################################################
    # Detailed log in Execution
    ############################################################
    def _get_task_executions_in_data_job(
        self,
    ) -> typing.List[typing.Optional[ExecutionItemTransport]]:
        """Gets all task executions in the given data job.

        Returns:
            A list containing all task executions.
        """
        execution_item_with_page_transport = IntegrationService.get_api_pools_pool_id_logs_job_id_executions(
            self.client, self.data_pool_id, self.id
        )
        execution_items = typing.cast(typing.List, execution_item_with_page_transport.execution_items)
        if execution_items is None or len(execution_items) == 0:
            return []
        last_execution_transport = execution_items[0]

        task_executions_transports = IntegrationService.get_api_pools_pool_id_logs_executions(
            self.client,
            self.data_pool_id,
            execution_id=last_execution_transport.execution_id,
            type_=ExecutionType.TASK,  # type: ignore
            id=last_execution_transport.job_id,
        )

        return task_executions_transports

    def _get_execution_detailed_error_log(self) -> typing.List["str"]:
        """Gets the detailed error log of given data job.

        Returns:
            A detailed error log.
        """
        detailed_error_log = []
        task_execution_transports = self._get_task_executions_in_data_job()
        for task_execution_transport in task_execution_transports:
            if task_execution_transport is not None and task_execution_transport.status == ExecutionStatus.FAIL:
                log_message_with_page_transport = IntegrationService.get_api_pools_pool_id_logs_executions_detail(
                    self.client,
                    self.data_pool_id,
                    execution_id=task_execution_transport.execution_id,
                    id=task_execution_transport.task_id,
                    type_=task_execution_transport.type_,
                )
                log_messages = typing.cast(typing.List, log_message_with_page_transport.log_messages)
                if log_messages is None or len(log_messages) == 0:
                    continue
                log_message_transport = log_messages[0]
                detailed_log = log_message_transport.log_message
                if detailed_log is not None:
                    detailed_error_log.append(detailed_log)
        return detailed_error_log
