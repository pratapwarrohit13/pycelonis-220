"""Module to interact with data models.

This module contains class to interact with a data model in EMS data integration.

Typical usage example:

    ```python
    data_model = data_pool.get_data_model(data_model_id)
    data_model.name = "NEW_NAME"
    data_model.update()
    data_model.reload()
    data_model.delete()
    ```
"""
import io
import logging
import pathlib
import typing

import pandas as pd
from pycelonis.config import Config
from pycelonis.ems.data_integration.data_export import DataExport
from pycelonis.ems.data_integration.data_model_table import DataModelTable
from pycelonis.ems.data_integration.foreign_key import ForeignKey
from pycelonis.ems.data_integration.process_configuration import ProcessConfiguration
from pycelonis.errors import (
    PyCelonisDataExportNotEnabledError,
    PyCelonisLoadInProgressError,
    PyCelonisReloadFailedError,
)
from pycelonis.pql import PQL
from pycelonis.service.integration.service import (
    DataCommand,
    DataExportRequest,
    DataModelConfiguration,
    DataModelDataLoadHistoryTransport,
    DataModelForeignKeyColumnTransport,
    DataModelForeignKeyTransport,
    DataModelLoadStatus,
    DataModelTableTransport,
    DataModelTransport,
    DataQuery,
    ExportType,
    IntegrationService,
    NameMappingTransport,
    QueryEnvironment,
)
from pycelonis.utils.parquet import read_parquet
from pycelonis.utils.polling import poll
from pycelonis_core.base.collection import CelonisCollection
from pycelonis_core.client.client import Client
from pycelonis_core.utils.errors import (
    PyCelonisHTTPStatusError,
    PyCelonisNotFoundError,
    PyCelonisTypeError,
    PyCelonisValueError,
)
from pydantic.v1 import Field

logger = logging.getLogger(__name__)


class DataModel(
    DataModelTransport,
):
    """Data model object to interact with data model specific data integration endpoints."""

    client: Client = Field(..., exclude=True)
    id: str
    """Id of data model."""
    pool_id: str
    """Id of data pool where data model is located."""
    name: typing.Optional[str]
    """Name of data model."""

    @classmethod
    def from_transport(cls, client: Client, data_model_transport: DataModelTransport) -> "DataModel":
        """Creates high-level data model object from given DataModelTransport.

        Args:
            client: Client to use to make API calls for given data model.
            data_model_transport: DataModelTransport object containing properties of data model.

        Returns:
            A DataModel object with properties from transport and given client.
        """
        return cls(client=client, **data_model_transport.dict())

    @property
    def data_pool_id(self) -> str:
        """Returns id of data pool for given data model.

        Returns:
            Id of data pool.
        """
        return self.pool_id

    @data_pool_id.setter
    def data_pool_id(self, data_pool_id: str) -> None:
        """Sets data pool id for given data model.

        Args:
            data_pool_id: New data pool id.
        """
        self.pool_id = data_pool_id

    def update(self) -> None:
        """Pushes local changes of data model to EMS and updates properties with response from EMS."""
        updated_data_model = IntegrationService.put_api_pools_pool_id_data_models_data_model_id(
            self.client, self.data_pool_id, self.id, self
        )
        logger.info("Successfully updated data model with id '%s'", self.id)
        self._update(updated_data_model)

    def sync(self) -> None:
        """Syncs data model properties with EMS."""
        synced_data_model = IntegrationService.get_api_pools_pool_id_data_models_data_model_id(
            self.client, self.data_pool_id, self.id
        )
        self._update(synced_data_model)

    def delete(self) -> None:
        """Deletes data model."""
        IntegrationService.delete_api_pools_pool_id_data_models_data_model_id(self.client, self.data_pool_id, self.id)
        logger.info("Successfully deleted data model with id '%s'", self.id)

    def __getter_attributes__(self) -> typing.List[str]:
        return ["process_configurations", "tables", "foreign_keys"]

    def __main_attributes__(self) -> typing.Optional[typing.List[str]]:
        return ["id", "name", "pool_id"]

    ############################################################
    # Data Model Reload
    ############################################################
    def reload(self, force_complete: bool = True, wait: bool = True) -> None:
        """Reloads given data model.

        Args:
            force_complete: If true, complete reload is triggered. Else reload from cache is triggered.
            wait: If true, function only returns once data model has been reloaded and raises error if reload fails.
                If false, function returns after triggering reload and does not raise errors in case load failed.

        Raises:
            PyCelonisLoadInProgressError: A data model reload is already in progress.
            PyCelonisReloadFailedError: Data model reload failed. Only triggered if `wait=True`.
        """
        self._reload(partial=False, wait=wait, force_complete=force_complete)

    def partial_reload(self, data_model_table_ids: typing.List[str], wait: bool = True) -> None:
        """Only reloads given tables in data model.

        Args:
            data_model_table_ids: List of data model table ids that needs to be reloaded.
            wait: If true, function only returns once data model tables have been reloaded and raises error if reload
                fails. If false, function returns after triggering partial reload and does not raise errors in case load
                failed.

        Raises:
            PyCelonisLoadInProgressError: A data model reload is already in progress.
            PyCelonisReloadFailedError: Data model reload failed. Only triggered if `wait=True`.
        """
        self._reload(partial=True, wait=wait, data_model_table_ids=data_model_table_ids)

    def get_load_status(self) -> typing.Optional[DataModelDataLoadHistoryTransport]:
        """Get the datamodel's load status.

        !!! api "API"
            - `GET: /integration/api/pools/{pool_id}/data-models/{datamodel_id}/load-history/load-info-sync`

        Returns:
            Load status.
        """
        load_info = IntegrationService.get_api_pools_pool_id_data_models_data_model_id_load_history_load_info_sync(
            self.client, self.data_pool_id, self.id
        ).load_info
        if not load_info:
            return None
        return load_info.current_compute_load

    def _reload(
        self,
        partial: bool,
        wait: bool,
        data_model_table_ids: typing.Optional[typing.List[str]] = None,
        **kwargs: typing.Any,
    ) -> None:
        self._verify_reload_possible()

        self._trigger_reload(partial, data_model_table_ids=data_model_table_ids, **kwargs)

        if wait:
            self._wait_for_reload()
            self._verify_load_successful()

    def _verify_reload_possible(self) -> None:
        if self._is_reload_in_progress(self.get_load_status()):
            raise PyCelonisLoadInProgressError

    def _trigger_reload(
        self,
        partial: bool,
        data_model_table_ids: typing.Optional[typing.List[str]],
        **kwargs: typing.Any,
    ) -> None:
        if partial:
            IntegrationService.post_api_v1_data_pools_pool_id_data_models_data_model_id_load_partial_sync(
                self.client, self.data_pool_id, self.id, data_model_table_ids or [], **kwargs  # type: ignore
            )
        else:
            IntegrationService.post_api_pools_pool_id_data_models_data_model_id_reload(
                self.client, self.data_pool_id, self.id, **kwargs
            )
        logger.info("Successfully triggered data model reload for data model with id '%s'", self.id)

    def _wait_for_reload(self) -> None:
        def is_reload_done(current_compute_load: typing.Optional[DataModelDataLoadHistoryTransport]) -> bool:
            return not self._is_reload_in_progress(current_compute_load)

        def format_current_compute_load(
            current_compute_load: typing.Optional[DataModelDataLoadHistoryTransport],
        ) -> str:
            if not current_compute_load:
                return ""

            formatted_compute_load_status = "Status:"
            if current_compute_load.load_status:
                formatted_compute_load_status += f" {current_compute_load.load_status.value}"
            if current_compute_load.message:
                formatted_compute_load_status += f" {current_compute_load.message}"

            return formatted_compute_load_status

        logger.info("Wait for execution of data model reload for data model with id '%s'", self.id)
        poll(
            target=self.get_load_status,
            wait_for=is_reload_done,
            message=format_current_compute_load,
            sleep=Config.POLLING_WAIT_SECONDS,
        )

    def _verify_load_successful(self) -> None:
        current_compute_load = self.get_load_status()
        if current_compute_load and current_compute_load.load_status not in [
            DataModelLoadStatus.SUCCESS,
            DataModelLoadStatus.WARNING,
        ]:
            raise PyCelonisReloadFailedError(current_compute_load.load_status, current_compute_load.message)

        if current_compute_load and current_compute_load.load_status == DataModelLoadStatus.WARNING:
            logger.warning("%s: %s", current_compute_load.load_status.value, current_compute_load.message)

    def _is_reload_in_progress(self, current_compute_load: typing.Optional[DataModelDataLoadHistoryTransport]) -> bool:
        if not current_compute_load:
            return False  # No previous load exists for data model

        return current_compute_load.load_status in [
            DataModelLoadStatus.RUNNING,
            DataModelLoadStatus.LOST_CONNECTION,
            DataModelLoadStatus.CANCELLING,
        ]

    ############################################################
    # Data Model Table
    ############################################################
    def add_table(self, name: str, alias: typing.Optional[str] = None, **kwargs: typing.Any) -> "DataModelTable":
        """Creates new data model with name in given data pool.

        Args:
            name: Name of existing table in data pool.
            alias: Alias of new data model table.
            **kwargs: Additional parameters set for
                [DataModelTableTransport][pycelonis.service.integration.service.DataModelTableTransport] object.

        Returns:
            A DataModelTable object for newly created data model table.

        Examples:
            Create a data model and add tables:
            ```python
            pool_table = data_pool.create_table(data_frame, "TEST_TABLE")
            data_model.add_table(name=pool_table.name, alias="ACTIVITIES")
            ```
        """
        data_model_table_transports = IntegrationService.post_api_pools_pool_id_data_model_data_model_id_tables(
            self.client,
            self.data_pool_id,
            self.id,
            [DataModelTableTransport(name=name, alias=alias, columns=[], **kwargs)],
        )

        if len(data_model_table_transports) != 1 or data_model_table_transports[0] is None:
            raise PyCelonisValueError(
                "Something went wrong while adding the table. Make sure the data model still exists."
            )

        logger.info("Successfully added data model table with id '%s' to data model", data_model_table_transports[0].id)
        return DataModelTable.from_transport(self.client, self.data_pool_id, data_model_table_transports[0])

    def get_table(self, id_: str) -> "DataModelTable":
        """Gets data model table with given id.

        Args:
             id_: Id of data model table.

        Returns:
            A DataModelTable object for data model table with given id.
        """
        data_model_table_transport = IntegrationService.get_api_pools_pool_id_data_model_data_model_id_tables_id(
            self.client, self.data_pool_id, self.id, id_
        )
        return DataModelTable.from_transport(self.client, self.data_pool_id, data_model_table_transport)

    def get_tables(self) -> CelonisCollection["DataModelTable"]:
        """Gets all data model tables of given data model.

        Returns:
            A list containing all data model tables.
        """
        data_model_table_transports = IntegrationService.get_api_pools_pool_id_data_model_data_model_id_tables(
            self.client, self.data_pool_id, self.id
        )
        return CelonisCollection(
            DataModelTable.from_transport(self.client, self.data_pool_id, data_model_table_transport)
            for data_model_table_transport in data_model_table_transports
            if data_model_table_transport is not None
        )

    ############################################################
    # Data Export
    ############################################################
    @staticmethod
    def export_data_frame_from(
        client: Client,
        pool_id: str,
        data_model_id: str,
        query: typing.Union[DataQuery, PQL],
        query_environment: typing.Optional[QueryEnvironment] = None,
    ) -> pd.DataFrame:
        """Creates new data export and downloads exported data as data frame for given data model.

        This method should be used in case you only have `USE ALL DATA MODELS` or `USE DATA MODEL` permissions.

        Args:
            client: Client to use to make API calls for data export.
            pool_id: Id of data pool.
            data_model_id: Id of data model.
            query: PQL query to export.
            query_environment: Query environment KPIs.

        Returns:
            A data frame containing the exported data.

        Examples:
            Export data into dataframe:
            ```python
            from pycelonis.pql import PQL, PQLColumn, PQLFilter, OrderByColumn
            from pycelonis.ems import DataModel

            query = PQL(distinct=False, limit=None, offset=None)
            query += PQLColumn(name="_CASE_KEY", query='"ACTIVITIES"."_CASE_KEY"')
            query += PQLColumn(name="ACTIVITY_EN", query='"ACTIVITIES"."ACTIVITY_EN"')

            df = DataModel.export_data_frame_from(
                client=celonis.client, pool_id=<data_pool_id>, data_model_id=<data_model_id>, query=query
            )
            ```
        """
        data_model = DataModel(client=client, pool_id=pool_id, id=data_model_id)
        return data_model.export_data_frame(query, query_environment=query_environment)

    def create_data_export(
        self,
        query: typing.Union[DataQuery, PQL],
        export_type: ExportType,
        query_environment: typing.Optional[QueryEnvironment] = None,
    ) -> "DataExport":
        """Creates new data export in given data model.

        Args:
            query: PQL query to export.
            export_type: Export type.
            query_environment: Query environment KPIs.

        Returns:
            A DataExport object for newly created data export.

        Examples:
            Manually run data export and wait for it to finish:
            ```python
            data_export = data_model.create_data_export(query=query, export_type=ExportType.PARQUET)
            data_export.wait_for_execution()
            chunks = data_export.get_chunks()

            for chunk in chunks:
                with open(f"<file_name>.parquet", "wb") as f:
                    f.write(chunk.read())
            ```
        """
        data_export_request = DataExportRequest(
            data_command=DataCommand(commands=[DataQuery(queries=query.queries)]),
            export_type=export_type,
            query_environment=query_environment,
        )

        try:
            data_export_transport = IntegrationService.post_api_v1_compute_data_model_id_export_query(
                self.client, self.id, data_export_request
            )
        except PyCelonisHTTPStatusError as e:
            raise PyCelonisDataExportNotEnabledError() from e

        logger.info("Successfully created data export with id '%s'", data_export_transport.id)
        return DataExport.from_transport(self.client, self.id, data_export_transport)

    def export_data_frame(
        self, query: typing.Union[DataQuery, PQL], query_environment: typing.Optional[QueryEnvironment] = None
    ) -> pd.DataFrame:
        """Creates new data export and downloads exported data as data frame.

        Args:
            query: PQL query to export.
            query_environment: Query environment KPIs.

        Returns:
            A data frame containing the exported data.

        Examples:
            Export data into dataframe:
            ```python
            from pycelonis.pql import PQL, PQLColumn, PQLFilter, OrderByColumn

            query = PQL(distinct=False, limit=None, offset=None)
            query += PQLColumn(name="_CASE_KEY", query='"ACTIVITIES"."_CASE_KEY"')
            query += PQLColumn(name="ACTIVITY_EN", query='"ACTIVITIES"."ACTIVITY_EN"')

            df = data_model.export_data_frame(query)
            ```
        """
        data_export = self.create_data_export(query, ExportType.PARQUET, query_environment=query_environment)

        data_export.wait_for_execution()

        data_frames = [read_parquet(chunk) for chunk in data_export.get_chunks()]
        return pd.concat(data_frames).reset_index(drop=True)

    ############################################################
    # Foreign Key
    ############################################################
    def create_foreign_key(
        self, source_table_id: str, target_table_id: str, columns: typing.List[typing.Tuple[str, str]]
    ) -> "ForeignKey":
        """Creates new foreign key with name in given data model.

        Args:
            source_table_id: Id of source table
            target_table_id: Id of target table
            columns: List of 2D-tuples consisting of a 'sourceColumnName' and 'targetColumnName' which represents
                the foreign_key, e.g. columns=[('Col1', 'Col3'), ('Col2', 'Col2'), ..]

        Returns:
            A ForeignKey object for newly created foreign key.

        Examples:
            Create foreign key between two tables:
            ```python
            ekpo = tables.find("EKPO")
            activities = tables.find("ACTIVITIES")

            foreign_key = data_model.create_foreign_key(
                source_table_id=ekpo.id,
                target_table_id=activities.id,
                columns=[("_CASE_KEY", "_CASE_KEY")]
            )
            ```
        """
        self._verify_columns_format(columns)

        foreign_key_transport = IntegrationService.post_api_pools_pool_id_data_models_data_model_id_foreign_keys(
            self.client,
            self.data_pool_id,
            self.id,
            DataModelForeignKeyTransport(
                source_table_id=source_table_id,
                target_table_id=target_table_id,
                columns=[
                    DataModelForeignKeyColumnTransport(source_column_name=col[0], target_column_name=col[1])
                    for col in columns
                ],
            ),
        )
        logger.info("Successfully created foreign key with id '%s'", foreign_key_transport.id)
        return ForeignKey.from_transport(self.client, self.data_pool_id, foreign_key_transport)

    def get_foreign_key(self, id_: str) -> "ForeignKey":
        """Gets foreign key with given id.

        Args:
            id_: Id of foreign key.

        Returns:
            A ForeignKey object for foreign key with given id.
        """
        foreign_key_transport = IntegrationService.get_api_pools_pool_id_data_models_data_model_id_foreign_keys_id(
            self.client, self.data_pool_id, self.id, id_
        )
        return ForeignKey.from_transport(self.client, self.data_pool_id, foreign_key_transport)

    def get_foreign_keys(self) -> CelonisCollection["ForeignKey"]:
        """Gets all foreign keys of given data pool.

        Returns:
            A list containing all foreign keys.
        """
        foreign_key_transports = IntegrationService.get_api_pools_pool_id_data_models_data_model_id_foreign_keys(
            self.client, self.data_pool_id, self.id
        )
        return CelonisCollection(
            ForeignKey.from_transport(self.client, self.data_pool_id, foreign_key_transport)
            for foreign_key_transport in foreign_key_transports
            if foreign_key_transport is not None
        )

    def _verify_columns_format(self, columns: typing.List[typing.Tuple[str, str]]) -> None:
        """Verifies that columns are passed properly for foreign key."""
        for col in columns:
            if len(col) != 2:
                raise PyCelonisTypeError(
                    "Columns need to have format [('source_col1', 'target_col1'), ('source_col2', 'target_col2'), ..]"
                )

    ############################################################
    # Name Mappings
    ############################################################
    def add_name_mappings(
        self, file_path: typing.Union[str, pathlib.Path]
    ) -> typing.List[typing.Optional["NameMappingTransport"]]:
        """Add the name mappings to data model.

        Args:
            file_path: Path of name mapping Excel file.

        Returns:
            List of name mappings.
        """
        with open(file_path, "rb") as byte_file:
            self._upload_name_mappings(byte_file)

        logger.info("Successfully added name mappings for data pool with id '%s'", self.data_pool_id)
        return self.get_name_mappings()

    def get_name_mappings(self) -> typing.List[typing.Optional["NameMappingTransport"]]:
        """Gets name mappings of given data model.

        Returns:
            A list containing all name mappings.
        """
        return IntegrationService.get_api_pools_pool_id_data_models_data_model_id_name_mapping(
            self.client, self.data_pool_id, self.id
        )

    def delete_name_mappings(self) -> None:
        """Deletes name mappings of given data model."""
        IntegrationService.delete_api_pools_pool_id_data_models_data_model_id_name_mapping(
            self.client, self.data_pool_id, self.id
        )
        logger.info("Successfully deleted name mappings for data pool with id '%s'", self.data_pool_id)

    def _upload_name_mappings(self, file: io.BufferedReader) -> None:
        """Upload the Datamodels Name Mappings.

        Args:
            file: File stream to be uploaded.
        """
        IntegrationService.post_api_pools_pool_id_data_models_data_model_id_name_mapping_file(
            self.client, self.data_pool_id, self.id, {"file": file}
        )

    ############################################################
    # Process Configuration
    ############################################################
    def create_process_configuration(
        self,
        activity_table_id: str,
        case_id_column: str,
        activity_column: str,
        timestamp_column: str,
        sorting_column: typing.Optional[str] = None,
        case_table_id: typing.Optional[str] = None,
        **kwargs: typing.Any,
    ) -> "ProcessConfiguration":
        """Creates new process configuration in given data model.

        Args:
            activity_table_id: Id of activity table.
            case_id_column: Column name of case id column.
            activity_column: Column name of activity column.
            timestamp_column: Column name of timestamp column.
            sorting_column: Column name of sorting column.
            case_table_id: Id of case table.
            **kwargs: Additional parameters set for
                [DataModelConfiguration][pycelonis.service.integration.service.DataModelConfiguration] object.

        Returns:
            A ProcessConfiguration object for newly created process configuration.

        Examples:
            Create process configuration for given data model:
            ```python
            ekpo = tables.find("EKPO")
            activities = tables.find("ACTIVITIES")

            process_configuration = data_model.create_process_configuration(
                activity_table_id=activities.id,
                case_id_column="_CASE_KEY",
                activity_column="ACTIVITY_EN",
                timestamp_column="EVENTTIME",
                sorting_column="_SORTING",
                case_table_id=ekpo.id
            )
            ```
        """
        data_model_configuration_transport = (
            IntegrationService.put_api_pools_pool_id_data_models_data_model_id_process_configurations(
                self.client,
                self.data_pool_id,
                self.id,
                DataModelConfiguration(
                    activity_table_id=activity_table_id,
                    case_id_column=case_id_column,
                    activity_column=activity_column,
                    timestamp_column=timestamp_column,
                    sorting_column=sorting_column,
                    case_table_id=case_table_id,
                    **kwargs,
                ),
            )
        )
        logger.info("Successfully created process configuration with id '%s'", data_model_configuration_transport.id)
        return ProcessConfiguration.from_transport(self.client, self.data_pool_id, data_model_configuration_transport)

    def get_process_configuration(self, id_: str) -> "ProcessConfiguration":
        """Gets process configuration with given id.

        Args:
             id_: Id of process configuration.

        Returns:
            A ProcessConfiguration object for process configuration with given id.

        Raises:
            PyCelonisNotFoundError: Raised if no process configuration with given id exists
        """
        for config in self.get_process_configurations():
            if config.id == id_:
                return config

        raise PyCelonisNotFoundError(f"No process configuration with id '{id_}' found in data model.")

    def get_process_configurations(self) -> CelonisCollection["ProcessConfiguration"]:
        """Gets all process configurations of given data model.

        Returns:
            A list containing all process configurations.
        """
        data_model_configuration_transports = (
            IntegrationService.get_api_pools_pool_id_data_models_data_model_id_process_configurations(
                self.client, self.data_pool_id, self.id
            )
        )
        return CelonisCollection(
            ProcessConfiguration.from_transport(self.client, self.data_pool_id, data_model_configuration_transport)
            for data_model_configuration_transport in data_model_configuration_transports
            if data_model_configuration_transport is not None
        )

