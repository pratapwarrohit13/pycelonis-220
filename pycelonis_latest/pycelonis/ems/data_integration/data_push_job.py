"""Module to interact with data push jobs.

This module contains class to interact with a data push job in EMS data integration.

Typical usage example:

```python
data_push_job = data_pool.create_data_push_job(
    "TABLE_NAME",
    JobType.REPLACE,
)
data_push_job.add_file_chunk(
    file
)
data_push_job.execute()
```
"""

import io
import logging
import typing

import pandas as pd
from pandas.api.types import infer_dtype
from pyarrow import ArrowTypeError
from pycelonis.config import Config
from pycelonis.errors import PyCelonisDataPushExecutionFailedError, PyCelonisDataPushJobNotNew
from pycelonis.service.data_ingestion.service import DataIngestionService, DataPushChunk
from pycelonis.service.data_ingestion.service import DataPushJob as DataPushJobBase
from pycelonis.service.data_ingestion.service import JobStatus
from pycelonis.utils.parquet import write_parquet
from pycelonis.utils.polling import poll
from pycelonis_core.base.collection import CelonisCollection
from pycelonis_core.client.client import Client
from pycelonis_core.utils.errors import PyCelonisValueError
from tqdm.auto import tqdm

try:
    from pydantic.v1 import Field  # type: ignore
except ImportError:
    from pydantic import Field  # type: ignore


logger = logging.getLogger(__name__)


class DataPushJob(DataPushJobBase):
    """Data push job object to interact with data push job specific data integration endpoints."""

    client: Client = Field(..., exclude=True)
    id: str
    """Id of data push job."""
    data_pool_id: str
    """Id of data pool where data push job is located."""
    target_name: typing.Optional[str]
    """Name of table where data is pushed to."""
    connection_id: typing.Optional[str]
    """Id of data connection where data is pushed to."""

    @classmethod
    def from_transport(cls, client: Client, data_push_job_transport: DataPushJobBase) -> "DataPushJob":
        """Creates high-level data push job object from given DataPushJobTransport.

        Args:
            client: Client to use to make API calls for given data push job.
            data_push_job_transport: DataPushJobTransport object containing properties of data push job.

        Returns:
            A DataPushJob object with properties from transport and given client.
        """
        return cls(client=client, **data_push_job_transport.dict())

    def sync(self) -> None:
        """Syncs data push job properties with EMS."""
        synced_job = DataIngestionService.get_api_v1_data_push_pool_id_jobs_id(self.client, self.data_pool_id, self.id)
        self._update(synced_job)

    def delete(self) -> None:
        """Deletes data push job."""
        DataIngestionService.delete_api_v1_data_push_pool_id_jobs_id(self.client, self.data_pool_id, self.id)
        logger.info("Successfully deleted data push job with id '%s'", self.id)

    def __main_attributes__(self) -> typing.Optional[typing.List[str]]:
        return ["id", "keys", "target_name", "status"]

    ############################################################
    # Chunk
    ############################################################
    def add_file_chunk(self, file: typing.Union[io.BytesIO, io.BufferedReader]) -> None:
        """Adds file chunk to data push job.

        Args:
            file: File stream to be upserted within data push job.

        Examples:
            Create data push job to replace table:
            ```python
            data_push_job = data_pool.create_data_push_job(
                target_name="ACTIVITIES",
                type_=JobType.REPLACE,
            )

            with open(
                "ACTIVITIES.parquet",
                "rb",
            ) as file:
                data_push_job.add_file_chunk(
                    file
                )

            data_push_job.execute()
            ```
        """
        if isinstance(file, io.BufferedReader):
            file = io.BytesIO(file.read())

        DataIngestionService.post_api_v1_data_push_pool_id_jobs_id_chunks_upserted(
            self.client, self.data_pool_id, self.id, {"file": ("file", file)}
        )
        logger.info("Successfully upserted file chunk to data push job with id '%s'", self.id)

    def delete_file_chunk(self, file: typing.Union[io.BytesIO, io.BufferedReader]) -> None:
        """Deletes file chunk from data push job.

        Args:
            file: File stream to be deleted within data push job.

        Examples:
            Create data push job to delete table:
            ```python
            data_push_job = data_pool.create_data_push_job(
                target_name="ACTIVITIES",
                type_=JobType.DELTA,
                keys=[
                    "_CASE_KEY"
                ],
            )

            with open(
                "ACTIVITIES.parquet",
                "rb",
            ) as file:
                data_push_job.delete_file_chunk(
                    file
                )

            data_push_job.execute()
            ```
        """
        if isinstance(file, io.BufferedReader):
            file = io.BytesIO(file.read())

        DataIngestionService.post_api_v1_data_push_pool_id_jobs_id_chunks_deleted(
            self.client, self.data_pool_id, self.id, {"file": ("file", file)}
        )

        logger.info("Successfully added delete file chunk to data push job with id '%s'", self.id)

    def add_data_frame(self, df: pd.DataFrame, chunk_size: int = 100_000, index: typing.Optional[bool] = False) -> None:
        """Splits data frame into chunks of size `chunk_size` and adds each chunk to data push job.

        Args:
            df: Data frame to push with given data push job.
            chunk_size: Number of rows for each chunk.
            index: Whether index is included in parquet file. See
                [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html).

        Examples:
            Create data push job to replace table:
            ```python
            data_push_job = data_pool.create_data_push_job(
                target_name="ACTIVITIES",
                type_=JobType.REPLACE,
            )

            df = pd.DataFrame(
                {
                    "ID": [
                        "aa",
                        "bb",
                        "cc",
                    ],
                    "TEST_COLUMN": [
                        1,
                        2,
                        3,
                    ],
                }
            )
            data_push_job.add_data_frame(
                df
            )

            data_push_job.execute()
            ```
        """
        if df.empty:
            raise ValueError("Can't add empty data frame.")

        logger.info("Add data frame as file chunks to data push job with id '%s'", self.id)

        df = self._cast_columns_to_inferred(df)

        for file_chunk in self._get_file_chunks(df, chunk_size, index):
            self.add_file_chunk(file_chunk)

    def delete_data_frame(
        self, df: pd.DataFrame, chunk_size: int = 100_000, index: typing.Optional[bool] = False
    ) -> None:
        """Splits data frame into chunks of size `chunk_size` and deletes each chunk to data push job.

        Args:
            df: Data frame to push with given data push job.
            chunk_size: Number of rows for each chunk.
            index: Whether index is included in parquet file. See
                [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html).

        Examples:
            Create data push job to delete table:
            ```python
            data_push_job = data_pool.create_data_push_job(
                target_name="ACTIVITIES",
                type_=JobType.DELTA,
                keys=[
                    "_CASE_KEY"
                ],
            )

            df = pd.DataFrame(
                {
                    "_CASE_KEY": [
                        "aa",
                        "bb",
                        "cc",
                    ],
                    "TEST_COLUMN": [
                        1,
                        2,
                        3,
                    ],
                }
            )
            data_push_job.delete_data_frame(
                df
            )

            data_push_job.execute()
            ```
        """
        if df.empty:
            raise ValueError("Can't delete empty data frame.")

        logger.info("Delete data frame as file chunks to data push job with id '%s'", self.id)

        for file_chunk in self._get_file_chunks(df, chunk_size, index):
            self.delete_file_chunk(file_chunk)

    def _get_file_chunks(
        self, df: pd.DataFrame, chunk_size: int = 100_000, index: typing.Optional[bool] = False
    ) -> typing.Generator:
        for i in tqdm(range(0, df.shape[0], chunk_size), disable=Config.DISABLE_TQDM):
            with io.BytesIO() as file_chunk:
                try:
                    write_parquet(df.iloc[i : i + chunk_size], file_chunk, index=index)
                    file_chunk.seek(0)  # Reset position
                    yield file_chunk
                except ArrowTypeError as e:
                    raise PyCelonisValueError(
                        "Converting dataframe to parquet file failed. "
                        "Make sure dataframe only contains primitive types and no dicts or lists."
                    ) from e

    def get_chunks(self) -> CelonisCollection[typing.Optional[DataPushChunk]]:
        """Gets all chunks of given data push job.

        Returns:
            A list containing all chunks.
        """
        return CelonisCollection(
            DataIngestionService.get_api_v1_data_push_pool_id_jobs_id_chunks(self.client, self.data_pool_id, self.id)
        )

    def execute(self, wait: bool = True) -> None:
        """Execute given data push job.

        Args:
            wait: If true, function only returns once data push job has been executed and raises error if reload fails.
                If false, function returns after triggering execution and does not raise errors in case it failed.

        Raises:
            PyCelonisDataPushJobNotNew: Data push job has already been executed and can't be executed again.
            PyCelonisDataPushExecutionFailedError: Data push job execution failed. Only triggered if `wait=True`.

        Examples:
            Create data push job to replace table:
            ```python
            data_push_job = data_pool.create_data_push_job(
                target_name="ACTIVITIES",
                type_=JobType.REPLACE,
            )

            df = pd.DataFrame(
                {
                    "ID": [
                        "aa",
                        "bb",
                        "cc",
                    ],
                    "TEST_COLUMN": [
                        1,
                        2,
                        3,
                    ],
                }
            )
            data_push_job.add_data_frame(
                df
            )

            data_push_job.execute()
            ```
        """
        if not self._is_new(self._get_status()):
            raise PyCelonisDataPushJobNotNew

        DataIngestionService.post_api_v1_data_push_pool_id_jobs_id(self.client, self.data_pool_id, self.id)
        logger.info("Successfully triggered execution for data push job with id '%s'", self.id)

        if wait:
            self._wait_for_execution()
            self._verify_execution_successful()

    def _cast_columns_to_inferred(self, df: pd.DataFrame) -> pd.DataFrame:
        """Casts the string and boolean columns of the df to inferred dtype for handling chunks with None occurrences.

        Args:
            df: df chunk to cast the columns to inferred type
        """
        dtypes_to_cast = ["string", "boolean"]
        dtypes_to_infer = ["object"]
        cols_to_infer = df.select_dtypes(include=dtypes_to_infer).columns  # type: ignore

        df_update = df.copy(deep=False)
        for col_name in cols_to_infer:
            inferred_dtype = infer_dtype(df_update[col_name])
            if inferred_dtype in dtypes_to_cast:
                df_update[col_name] = df_update[col_name].astype(inferred_dtype)  # type: ignore

        return df_update

    def _wait_for_execution(self) -> None:
        def is_done(data_push_job_transport: DataPushJobBase) -> bool:
            return data_push_job_transport.status in [JobStatus.ERROR, JobStatus.DONE]

        def format_status(data_push_job_transport: DataPushJobBase) -> str:
            if not data_push_job_transport.status:
                return ""
            return f"Status: {data_push_job_transport.status}"

        logger.info("Wait for execution of data push job with id '%s'", self.id)
        poll(
            target=self._get_status,
            wait_for=is_done,
            message=format_status,
            sleep=Config.POLLING_WAIT_SECONDS,
        )

    def _verify_execution_successful(self) -> None:
        job_status = self._get_status()
        if job_status.status != JobStatus.DONE:
            raise PyCelonisDataPushExecutionFailedError(job_status.status, job_status.logs)

    def _get_status(self) -> DataPushJobBase:
        return DataIngestionService.get_api_v1_data_push_pool_id_jobs_id(self.client, self.data_pool_id, self.id)

    def _is_new(self, data_push_job_transport: DataPushJobBase) -> bool:
        return data_push_job_transport.status == JobStatus.NEW
